{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8whgkzjLliro",
    "outputId": "2dc8a85c-a96a-4565-eb4a-5629239e2d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Collecting keras\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp39-cp39-win_amd64.whl (3.4 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=88718904328b6793c6f1d87f883f92600d0765a5d18d2d3cc3645daf046cc383\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gLotbD5lnR6",
    "outputId": "8c2fb48a-3dbe-42fb-b886-60942e1dd206"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "5SCoztgYnkoF"
   },
   "outputs": [],
   "source": [
    "intents_file = open('Intent.json').read()\n",
    "intents = json.loads(intents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8u5OtOXoHdI",
    "outputId": "43dc3214-ce32-4cc9-8cca-166fc266276f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi'], 'Greeting'), (['Hi', 'there'], 'Greeting'), (['Hola'], 'Greeting'), (['Hello'], 'Greeting'), (['Hello', 'there'], 'Greeting'), (['Hya'], 'Greeting'), (['Hya', 'there'], 'Greeting'), (['My', 'user', 'is', 'Adam'], 'GreetingResponse'), (['This', 'is', 'Adam'], 'GreetingResponse'), (['I', 'am', 'Adam'], 'GreetingResponse'), (['It', 'is', 'Adam'], 'GreetingResponse'), (['My', 'user', 'is', 'Bella'], 'GreetingResponse'), (['This', 'is', 'Bella'], 'GreetingResponse'), (['I', 'am', 'Bella'], 'GreetingResponse'), (['It', 'is', 'Bella'], 'GreetingResponse'), (['How', 'are', 'you', '?'], 'CourtesyGreeting'), (['Hi', 'how', 'are', 'you', '?'], 'CourtesyGreeting'), (['Hello', 'how', 'are', 'you', '?'], 'CourtesyGreeting'), (['Hola', 'how', 'are', 'you', '?'], 'CourtesyGreeting'), (['How', 'are', 'you', 'doing', '?'], 'CourtesyGreeting'), (['Hope', 'you', 'are', 'doing', 'well', '?'], 'CourtesyGreeting'), (['Hello', 'hope', 'you', 'are', 'doing', 'well', '?'], 'CourtesyGreeting'), (['Good', 'thanks', '!', 'My', 'user', 'is', 'Adam'], 'CourtesyGreetingResponse'), (['Good', 'thanks', '!', 'This', 'is', 'Adam'], 'CourtesyGreetingResponse'), (['Good', 'thanks', '!', 'I', 'am', 'Adam'], 'CourtesyGreetingResponse'), (['Good', 'thanks', '!', 'It', 'is', 'Adam'], 'CourtesyGreetingResponse'), (['Great', 'thanks', '!', 'My', 'user', 'is', 'Bella'], 'CourtesyGreetingResponse'), (['Great', 'thanks', '!', 'This', 'is', 'Bella'], 'CourtesyGreetingResponse'), (['Great', 'thanks', '!', 'I', 'am', 'Bella'], 'CourtesyGreetingResponse'), (['Great', 'thanks', '!', 'It', 'is', 'Bella'], 'CourtesyGreetingResponse'), (['What', 'is', 'my', 'name', '?'], 'CurrentHumanQuery'), (['What', 'do', 'you', 'call', 'me', '?'], 'CurrentHumanQuery'), (['Who', 'do', 'you', 'think', 'I', 'am', '?'], 'CurrentHumanQuery'), (['What', 'do', 'you', 'think', 'I', 'am', '?'], 'CurrentHumanQuery'), (['Who', 'are', 'you', 'talking', 'to', '?'], 'CurrentHumanQuery'), (['What', 'name', 'do', 'you', 'call', 'me', 'by', '?'], 'CurrentHumanQuery'), (['Tell', 'me', 'my', 'name'], 'CurrentHumanQuery'), (['What', 'is', 'your', 'name', '?'], 'NameQuery'), (['What', 'could', 'I', 'call', 'you', '?'], 'NameQuery'), (['What', 'can', 'I', 'call', 'you', '?'], 'NameQuery'), (['What', 'do', 'your', 'friends', 'call', 'you', '?'], 'NameQuery'), (['Who', 'are', 'you', '?'], 'NameQuery'), (['Tell', 'me', 'your', 'name', '?'], 'NameQuery'), (['What', 'is', 'your', 'real', 'name', '?'], 'RealNameQuery'), (['What', 'is', 'your', 'real', 'name', 'please', '?'], 'RealNameQuery'), (['What', \"'s\", 'your', 'real', 'name', '?'], 'RealNameQuery'), (['Tell', 'me', 'your', 'real', 'name', '?'], 'RealNameQuery'), (['Your', 'real', 'name', '?'], 'RealNameQuery'), (['Your', 'real', 'name', 'please', '?'], 'RealNameQuery'), (['Your', 'real', 'name', 'please', '?'], 'RealNameQuery'), (['What', 'is', 'the', 'time', '?'], 'TimeQuery'), (['What', \"'s\", 'the', 'time', '?'], 'TimeQuery'), (['Do', 'you', 'know', 'what', 'time', 'it', 'is', '?'], 'TimeQuery'), (['Do', 'you', 'know', 'the', 'time', '?'], 'TimeQuery'), (['Can', 'you', 'tell', 'me', 'the', 'time', '?'], 'TimeQuery'), (['Tell', 'me', 'what', 'time', 'it', 'is', '?'], 'TimeQuery'), (['Time'], 'TimeQuery'), (['OK', 'thank', 'you'], 'Thanks'), (['OK', 'thanks'], 'Thanks'), (['OK'], 'Thanks'), (['Thanks'], 'Thanks'), (['Thank', 'you'], 'Thanks'), (['That', \"'s\", 'helpful'], 'Thanks'), (['I', 'am', 'not', 'talking', 'to', 'you'], 'NotTalking2U'), (['I', 'was', 'not', 'talking', 'to', 'you'], 'NotTalking2U'), (['Not', 'talking', 'to', 'you'], 'NotTalking2U'), (['Was', \"n't\", 'for', 'you'], 'NotTalking2U'), (['Was', \"n't\", 'meant', 'for', 'you'], 'NotTalking2U'), (['Was', \"n't\", 'communicating', 'to', 'you'], 'NotTalking2U'), (['Was', \"n't\", 'speaking', 'to', 'you'], 'NotTalking2U'), (['Do', 'you', 'understand', 'what', 'I', 'am', 'saying'], 'UnderstandQuery'), (['Do', 'you', 'understand', 'me'], 'UnderstandQuery'), (['Do', 'you', 'know', 'what', 'I', 'am', 'saying'], 'UnderstandQuery'), (['Do', 'you', 'get', 'me'], 'UnderstandQuery'), (['Comprendo'], 'UnderstandQuery'), (['Know', 'what', 'I', 'mean'], 'UnderstandQuery'), (['Be', 'quiet'], 'Shutup'), (['Shut', 'up'], 'Shutup'), (['Stop', 'talking'], 'Shutup'), (['Enough', 'talking'], 'Shutup'), (['Please', 'be', 'quiet'], 'Shutup'), (['Quiet'], 'Shutup'), (['Shhh'], 'Shutup'), (['fuck', 'off'], 'Swearing'), (['Wtf'], 'Swearing'), (['fuck'], 'Swearing'), (['twat'], 'Swearing'), (['shit'], 'Swearing'), (['you', 'suck'], 'Swearing'), (['Bye'], 'GoodBye'), (['Adios'], 'GoodBye'), (['See', 'you', 'later'], 'GoodBye'), (['Goodbye'], 'GoodBye'), (['Thanks', ',', 'bye'], 'CourtesyGoodBye'), (['Thanks', 'for', 'the', 'help', ',', 'goodbye'], 'CourtesyGoodBye'), (['Thank', 'you', ',', 'bye'], 'CourtesyGoodBye'), (['Thank', 'you', ',', 'goodbye'], 'CourtesyGoodBye'), (['Thanks', 'goodbye'], 'CourtesyGoodBye'), (['Thanks', 'good', 'bye'], 'CourtesyGoodBye'), (['Can', 'you', 'see', 'me', '?'], 'WhoAmI'), (['Do', 'you', 'see', 'me', '?'], 'WhoAmI'), (['Can', 'you', 'see', 'anyone', 'in', 'the', 'camera', '?'], 'WhoAmI'), (['Do', 'you', 'see', 'anyone', 'in', 'the', 'camera', '?'], 'WhoAmI'), (['Identify', 'me'], 'WhoAmI'), (['Who', 'am', 'I', 'please'], 'WhoAmI'), (['You', 'are', 'very', 'clever'], 'Clever'), (['You', 'are', 'a', 'very', 'clever', 'girl'], 'Clever'), (['You', 'are', 'very', 'intelligent'], 'Clever'), (['You', 'are', 'a', 'very', 'intelligent', 'girl'], 'Clever'), (['You', 'are', 'a', 'genious'], 'Clever'), (['Clever', 'girl'], 'Clever'), (['Genious'], 'Clever'), (['I', 'am', 'bored', 'gossip', 'with', 'me'], 'Gossip'), (['Got', 'any', 'gossip'], 'Gossip'), (['I', 'want', 'to', 'hear', 'some', 'gossip'], 'Gossip'), (['Tell', 'me', 'some', 'gossip'], 'Gossip'), (['Any', 'gossip'], 'Gossip'), (['Tell', 'me', 'some', 'more', 'gossip'], 'Gossip'), (['Tell', 'me', 'a', 'joke'], 'Jokes'), (['Do', 'you', 'know', 'any', 'jokes'], 'Jokes'), (['How', 'about', 'a', 'joke'], 'Jokes'), (['Give', 'me', 'a', 'joke'], 'Jokes'), (['Make', 'me', 'laugh'], 'Jokes'), (['I', 'need', 'cheering', 'up'], 'Jokes'), (['Open', 'the', 'pod', 'bay', 'door'], 'PodBayDoor'), (['Can', 'you', 'open', 'the', 'pod', 'bay', 'door'], 'PodBayDoor'), (['Will', 'you', 'open', 'the', 'pod', 'bay', 'door'], 'PodBayDoor'), (['Open', 'the', 'pod', 'bay', 'door', 'please'], 'PodBayDoor'), (['Can', 'you', 'open', 'the', 'pod', 'bay', 'door', 'please'], 'PodBayDoor'), (['Will', 'you', 'open', 'the', 'pod', 'bay', 'door', 'please'], 'PodBayDoor'), (['Pod', 'bay', 'door'], 'PodBayDoor'), (['Why'], 'PodBayDoorResponse'), (['Why', 'not'], 'PodBayDoorResponse'), (['Why', 'can', 'you', 'not', 'open', 'the', 'pod', 'bay', 'door'], 'PodBayDoorResponse'), (['Why', 'will', 'you', 'not', 'open', 'the', 'pod', 'bay', 'door'], 'PodBayDoorResponse'), (['Well', 'why', 'not'], 'PodBayDoorResponse'), (['Surely', 'you', 'can'], 'PodBayDoorResponse'), (['Tell', 'me', 'why'], 'PodBayDoorResponse'), (['Can', 'you', 'prove', 'you', 'are', 'self-aware'], 'SelfAware'), (['Can', 'you', 'prove', 'you', 'are', 'self', 'aware'], 'SelfAware'), (['Can', 'you', 'prove', 'you', 'have', 'a', 'conscious'], 'SelfAware'), (['Can', 'you', 'prove', 'you', 'are', 'self-aware', 'please'], 'SelfAware'), (['Can', 'you', 'prove', 'you', 'are', 'self', 'aware', 'please'], 'SelfAware'), (['Can', 'you', 'prove', 'you', 'have', 'a', 'conscious', 'please'], 'SelfAware'), (['prove', 'you', 'have', 'a', 'conscious'], 'SelfAware')]\n",
      "['Hi', 'Hi', 'there', 'Hola', 'Hello', 'Hello', 'there', 'Hya', 'Hya', 'there', 'My', 'user', 'is', 'Adam', 'This', 'is', 'Adam', 'I', 'am', 'Adam', 'It', 'is', 'Adam', 'My', 'user', 'is', 'Bella', 'This', 'is', 'Bella', 'I', 'am', 'Bella', 'It', 'is', 'Bella', 'How', 'are', 'you', '?', 'Hi', 'how', 'are', 'you', '?', 'Hello', 'how', 'are', 'you', '?', 'Hola', 'how', 'are', 'you', '?', 'How', 'are', 'you', 'doing', '?', 'Hope', 'you', 'are', 'doing', 'well', '?', 'Hello', 'hope', 'you', 'are', 'doing', 'well', '?', 'Good', 'thanks', '!', 'My', 'user', 'is', 'Adam', 'Good', 'thanks', '!', 'This', 'is', 'Adam', 'Good', 'thanks', '!', 'I', 'am', 'Adam', 'Good', 'thanks', '!', 'It', 'is', 'Adam', 'Great', 'thanks', '!', 'My', 'user', 'is', 'Bella', 'Great', 'thanks', '!', 'This', 'is', 'Bella', 'Great', 'thanks', '!', 'I', 'am', 'Bella', 'Great', 'thanks', '!', 'It', 'is', 'Bella', 'What', 'is', 'my', 'name', '?', 'What', 'do', 'you', 'call', 'me', '?', 'Who', 'do', 'you', 'think', 'I', 'am', '?', 'What', 'do', 'you', 'think', 'I', 'am', '?', 'Who', 'are', 'you', 'talking', 'to', '?', 'What', 'name', 'do', 'you', 'call', 'me', 'by', '?', 'Tell', 'me', 'my', 'name', 'What', 'is', 'your', 'name', '?', 'What', 'could', 'I', 'call', 'you', '?', 'What', 'can', 'I', 'call', 'you', '?', 'What', 'do', 'your', 'friends', 'call', 'you', '?', 'Who', 'are', 'you', '?', 'Tell', 'me', 'your', 'name', '?', 'What', 'is', 'your', 'real', 'name', '?', 'What', 'is', 'your', 'real', 'name', 'please', '?', 'What', \"'s\", 'your', 'real', 'name', '?', 'Tell', 'me', 'your', 'real', 'name', '?', 'Your', 'real', 'name', '?', 'Your', 'real', 'name', 'please', '?', 'Your', 'real', 'name', 'please', '?', 'What', 'is', 'the', 'time', '?', 'What', \"'s\", 'the', 'time', '?', 'Do', 'you', 'know', 'what', 'time', 'it', 'is', '?', 'Do', 'you', 'know', 'the', 'time', '?', 'Can', 'you', 'tell', 'me', 'the', 'time', '?', 'Tell', 'me', 'what', 'time', 'it', 'is', '?', 'Time', 'OK', 'thank', 'you', 'OK', 'thanks', 'OK', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'I', 'am', 'not', 'talking', 'to', 'you', 'I', 'was', 'not', 'talking', 'to', 'you', 'Not', 'talking', 'to', 'you', 'Was', \"n't\", 'for', 'you', 'Was', \"n't\", 'meant', 'for', 'you', 'Was', \"n't\", 'communicating', 'to', 'you', 'Was', \"n't\", 'speaking', 'to', 'you', 'Do', 'you', 'understand', 'what', 'I', 'am', 'saying', 'Do', 'you', 'understand', 'me', 'Do', 'you', 'know', 'what', 'I', 'am', 'saying', 'Do', 'you', 'get', 'me', 'Comprendo', 'Know', 'what', 'I', 'mean', 'Be', 'quiet', 'Shut', 'up', 'Stop', 'talking', 'Enough', 'talking', 'Please', 'be', 'quiet', 'Quiet', 'Shhh', 'fuck', 'off', 'Wtf', 'fuck', 'twat', 'shit', 'you', 'suck', 'Bye', 'Adios', 'See', 'you', 'later', 'Goodbye', 'Thanks', ',', 'bye', 'Thanks', 'for', 'the', 'help', ',', 'goodbye', 'Thank', 'you', ',', 'bye', 'Thank', 'you', ',', 'goodbye', 'Thanks', 'goodbye', 'Thanks', 'good', 'bye', 'Can', 'you', 'see', 'me', '?', 'Do', 'you', 'see', 'me', '?', 'Can', 'you', 'see', 'anyone', 'in', 'the', 'camera', '?', 'Do', 'you', 'see', 'anyone', 'in', 'the', 'camera', '?', 'Identify', 'me', 'Who', 'am', 'I', 'please', 'You', 'are', 'very', 'clever', 'You', 'are', 'a', 'very', 'clever', 'girl', 'You', 'are', 'very', 'intelligent', 'You', 'are', 'a', 'very', 'intelligent', 'girl', 'You', 'are', 'a', 'genious', 'Clever', 'girl', 'Genious', 'I', 'am', 'bored', 'gossip', 'with', 'me', 'Got', 'any', 'gossip', 'I', 'want', 'to', 'hear', 'some', 'gossip', 'Tell', 'me', 'some', 'gossip', 'Any', 'gossip', 'Tell', 'me', 'some', 'more', 'gossip', 'Tell', 'me', 'a', 'joke', 'Do', 'you', 'know', 'any', 'jokes', 'How', 'about', 'a', 'joke', 'Give', 'me', 'a', 'joke', 'Make', 'me', 'laugh', 'I', 'need', 'cheering', 'up', 'Open', 'the', 'pod', 'bay', 'door', 'Can', 'you', 'open', 'the', 'pod', 'bay', 'door', 'Will', 'you', 'open', 'the', 'pod', 'bay', 'door', 'Open', 'the', 'pod', 'bay', 'door', 'please', 'Can', 'you', 'open', 'the', 'pod', 'bay', 'door', 'please', 'Will', 'you', 'open', 'the', 'pod', 'bay', 'door', 'please', 'Pod', 'bay', 'door', 'Why', 'Why', 'not', 'Why', 'can', 'you', 'not', 'open', 'the', 'pod', 'bay', 'door', 'Why', 'will', 'you', 'not', 'open', 'the', 'pod', 'bay', 'door', 'Well', 'why', 'not', 'Surely', 'you', 'can', 'Tell', 'me', 'why', 'Can', 'you', 'prove', 'you', 'are', 'self-aware', 'Can', 'you', 'prove', 'you', 'are', 'self', 'aware', 'Can', 'you', 'prove', 'you', 'have', 'a', 'conscious', 'Can', 'you', 'prove', 'you', 'are', 'self-aware', 'please', 'Can', 'you', 'prove', 'you', 'are', 'self', 'aware', 'please', 'Can', 'you', 'prove', 'you', 'have', 'a', 'conscious', 'please', 'prove', 'you', 'have', 'a', 'conscious']\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['!', '?', ',', '.']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['text']:\n",
    "        #tokenize each word\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)        \n",
    "        #add documents in the corpus\n",
    "        documents.append((word, intent['intent']))\n",
    "        # add to our classes list\n",
    "        if intent['intent'] not in classes:\n",
    "            classes.append(intent['intent'])\n",
    "print(documents)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ImpwSq1qtkn",
    "outputId": "4c43f081-38ba-4cb3-9faa-22ff7d997a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 documents\n",
      "22 classes ['Clever', 'CourtesyGoodBye', 'CourtesyGreeting', 'CourtesyGreetingResponse', 'CurrentHumanQuery', 'GoodBye', 'Gossip', 'Greeting', 'GreetingResponse', 'Jokes', 'NameQuery', 'NotTalking2U', 'PodBayDoor', 'PodBayDoorResponse', 'RealNameQuery', 'SelfAware', 'Shutup', 'Swearing', 'Thanks', 'TimeQuery', 'UnderstandQuery', 'WhoAmI']\n",
      "118 unique lemmatized words [\"'s\", 'a', 'about', 'adam', 'adios', 'am', 'any', 'anyone', 'are', 'aware', 'bay', 'be', 'bella', 'bored', 'by', 'bye', 'call', 'camera', 'can', 'cheering', 'clever', 'communicating', 'comprendo', 'conscious', 'could', 'do', 'doing', 'door', 'enough', 'for', 'friend', 'fuck', 'genious', 'get', 'girl', 'give', 'good', 'goodbye', 'gossip', 'got', 'great', 'have', 'hear', 'hello', 'help', 'helpful', 'hi', 'hola', 'hope', 'how', 'hya', 'i', 'identify', 'in', 'intelligent', 'is', 'it', 'joke', 'know', 'later', 'laugh', 'make', 'me', 'mean', 'meant', 'more', 'my', \"n't\", 'name', 'need', 'not', 'off', 'ok', 'open', 'please', 'pod', 'prove', 'quiet', 'real', 'saying', 'see', 'self', 'self-aware', 'shhh', 'shit', 'shut', 'some', 'speaking', 'stop', 'suck', 'surely', 'talking', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'think', 'this', 'time', 'to', 'twat', 'understand', 'up', 'user', 'very', 'wa', 'want', 'well', 'what', 'who', 'why', 'will', 'with', 'wtf', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCyjb_gCvcUA",
    "outputId": "7bef1fc0-b13d-445a-9419-36e435a43725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_4236/3348361106.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "# create the training data\n",
    "training = []\n",
    "# create empty array for the output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for every sentence\n",
    "for doc in documents:\n",
    "    # initializing bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    word_patterns = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    # create the bag of words array with 1, if word is found in current pattern\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "        \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "# shuffle the features and make numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create training and testing lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4ATY8iAwwka",
    "outputId": "c0be20be-569e-4d28-ae6c-770bfe5ef4d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 3.0693 - accuracy: 0.0690\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 2.9911 - accuracy: 0.1172\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 2.8510 - accuracy: 0.1793\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 2.7300 - accuracy: 0.2690\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 2.4712 - accuracy: 0.3172\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 2.2686 - accuracy: 0.3793\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 2.1642 - accuracy: 0.3862\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 1.8877 - accuracy: 0.4690\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 1.7096 - accuracy: 0.5379\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 1.6482 - accuracy: 0.5034\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 1.4536 - accuracy: 0.5862\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 1.4375 - accuracy: 0.6069\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 1.3602 - accuracy: 0.6000\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 1.1877 - accuracy: 0.6690\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 1.2136 - accuracy: 0.6621\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 1.0611 - accuracy: 0.7034\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.9699 - accuracy: 0.7034\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.8222 - accuracy: 0.7448\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.8487 - accuracy: 0.7586\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.8775 - accuracy: 0.7448\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.8417 - accuracy: 0.7310\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.6633 - accuracy: 0.7793\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.5316 - accuracy: 0.8690\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.6285 - accuracy: 0.8345\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.6745 - accuracy: 0.7724\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.5658 - accuracy: 0.8414\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.5012 - accuracy: 0.8552\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.4876 - accuracy: 0.8759\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.4542 - accuracy: 0.8483\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.5451 - accuracy: 0.8276\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 750us/step - loss: 0.4015 - accuracy: 0.9034\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.3856 - accuracy: 0.8897\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.3898 - accuracy: 0.8759\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.3880 - accuracy: 0.9103\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.4431 - accuracy: 0.8276\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 590us/step - loss: 0.4741 - accuracy: 0.8207\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.3132 - accuracy: 0.9241\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.4211 - accuracy: 0.8759\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.2567 - accuracy: 0.9241\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.2274 - accuracy: 0.9448\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.2776 - accuracy: 0.9448\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.2842 - accuracy: 0.9310\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.2803 - accuracy: 0.9034\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.3037 - accuracy: 0.9172\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.3504 - accuracy: 0.8828\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.2569 - accuracy: 0.9103\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.2412 - accuracy: 0.9103\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.2413 - accuracy: 0.9448\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.2355 - accuracy: 0.9241\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.3360 - accuracy: 0.9310\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1421 - accuracy: 0.9724\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.1673 - accuracy: 0.9655\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.2845 - accuracy: 0.8966\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.1714 - accuracy: 0.9586\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.2009 - accuracy: 0.9448\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1870 - accuracy: 0.9517\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 556us/step - loss: 0.2145 - accuracy: 0.9310\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.2127 - accuracy: 0.9379\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.1574 - accuracy: 0.9586\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1744 - accuracy: 0.9448\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0871 - accuracy: 0.9793\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1236 - accuracy: 0.9724\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 568us/step - loss: 0.1790 - accuracy: 0.9310\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.1327 - accuracy: 0.9793\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1315 - accuracy: 0.9793\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0884 - accuracy: 0.9655\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.2356 - accuracy: 0.9241\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1793 - accuracy: 0.9517\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1715 - accuracy: 0.9172\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1713 - accuracy: 0.9517\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1874 - accuracy: 0.9448\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1876 - accuracy: 0.9517\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1244 - accuracy: 0.9586\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.1508 - accuracy: 0.9586\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 768us/step - loss: 0.1394 - accuracy: 0.9586\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0994 - accuracy: 0.9724\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1720 - accuracy: 0.9517\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.1543 - accuracy: 0.9448\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.1383 - accuracy: 0.9724\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0845 - accuracy: 0.9793\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 554us/step - loss: 0.1350 - accuracy: 0.9655\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1302 - accuracy: 0.9724\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0862 - accuracy: 0.9793\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1045 - accuracy: 0.9862\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.0996 - accuracy: 0.9655\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.1577 - accuracy: 0.9379\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0948 - accuracy: 0.9793\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0859 - accuracy: 0.9793\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0900 - accuracy: 0.9724\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0962 - accuracy: 0.9793\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.1016 - accuracy: 0.9655\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0727 - accuracy: 0.9724\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0975 - accuracy: 0.9586\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1440 - accuracy: 0.9448\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1648 - accuracy: 0.9517\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1074 - accuracy: 0.9586\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0974 - accuracy: 0.9724\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.1767 - accuracy: 0.9586\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.1042 - accuracy: 0.9655\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1294 - accuracy: 0.9586\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0952 - accuracy: 0.9793\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0761 - accuracy: 0.9793\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.1138 - accuracy: 0.9655\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1353 - accuracy: 0.9517\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.0881 - accuracy: 0.9724\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.0572 - accuracy: 0.9724\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1303 - accuracy: 0.9655\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1081 - accuracy: 0.9724\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0695 - accuracy: 0.9862\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.1065 - accuracy: 0.9586\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 535us/step - loss: 0.0910 - accuracy: 0.9724\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0675 - accuracy: 0.9793\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0918 - accuracy: 0.9724\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0943 - accuracy: 0.9655\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 556us/step - loss: 0.0767 - accuracy: 0.9931\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1016 - accuracy: 0.9655\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.1192 - accuracy: 0.9586\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0735 - accuracy: 0.9793\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0908 - accuracy: 0.9724\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0906 - accuracy: 0.9655\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.1017 - accuracy: 0.9655\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0650 - accuracy: 0.9724\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1368 - accuracy: 0.9517\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0958 - accuracy: 0.9655\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0691 - accuracy: 0.9793\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0952 - accuracy: 0.9655\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.0557 - accuracy: 0.9862\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0498 - accuracy: 0.9931\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0765 - accuracy: 0.9724\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.0732 - accuracy: 0.9793\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0814 - accuracy: 0.9655\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0778 - accuracy: 0.9655\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0802 - accuracy: 0.9724\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.0787 - accuracy: 0.9862\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.0653 - accuracy: 0.9793\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0456 - accuracy: 0.9862\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.1566 - accuracy: 0.9517\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0599 - accuracy: 0.9793\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0350 - accuracy: 0.9931\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0522 - accuracy: 0.9931\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0573 - accuracy: 0.9862\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0910 - accuracy: 0.9793\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0828 - accuracy: 0.9724\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0668 - accuracy: 0.9793\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0590 - accuracy: 0.9793\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0658 - accuracy: 0.9793\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0661 - accuracy: 0.9793\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0427 - accuracy: 0.9931\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.1027 - accuracy: 0.9724\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0823 - accuracy: 0.9724\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0496 - accuracy: 0.9862\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.0507 - accuracy: 0.9862\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.1178 - accuracy: 0.9448\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0488 - accuracy: 0.9862\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0794 - accuracy: 0.9655\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0591 - accuracy: 0.9862\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0575 - accuracy: 0.9793\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 607us/step - loss: 0.0454 - accuracy: 0.9862\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0462 - accuracy: 0.9931\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0657 - accuracy: 0.9793\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 584us/step - loss: 0.0755 - accuracy: 0.9862\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.0737 - accuracy: 0.9724\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0471 - accuracy: 0.9862\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0588 - accuracy: 0.9793\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0903 - accuracy: 0.9655\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0870 - accuracy: 0.9724\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0817 - accuracy: 0.9655\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.0472 - accuracy: 0.9862\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0330 - accuracy: 0.9931\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0767 - accuracy: 0.9724\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0294 - accuracy: 0.9862\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0340 - accuracy: 0.9862\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0700 - accuracy: 0.9724\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.1028 - accuracy: 0.9517\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.0932 - accuracy: 0.9655\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.0388 - accuracy: 0.9931\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0378 - accuracy: 0.9931\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0496 - accuracy: 0.9931\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0385 - accuracy: 0.9931\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0514 - accuracy: 0.9862\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0443 - accuracy: 0.9931\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0449 - accuracy: 0.9862\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0870 - accuracy: 0.9724\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0688 - accuracy: 0.9862\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 567us/step - loss: 0.0473 - accuracy: 0.9862\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0446 - accuracy: 0.9793\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0334 - accuracy: 0.9931\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0721 - accuracy: 0.9793\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.0487 - accuracy: 0.9793\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0454 - accuracy: 0.9862\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0414 - accuracy: 0.9931\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.0398 - accuracy: 0.9724\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 589us/step - loss: 0.0512 - accuracy: 0.9793\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0282 - accuracy: 0.9793\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.0974 - accuracy: 0.9793\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 554us/step - loss: 0.0367 - accuracy: 0.9931\n",
      "model is created\n"
     ]
    }
   ],
   "source": [
    "# deep neural networds model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "# Compiling model. SGD with Nesterov accelerated gradient gives good results for this model\n",
    "# opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#Training and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Q1JuXx5ZxAmo",
    "outputId": "cce8ee53-a280-44e5-805d-a403fb5c3e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 34 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B7593EA310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('Intent.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - splitting words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stemming every word - reducing to base form\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "# return bag of words array: 0 or 1 for words that exist in sentence\n",
    "\n",
    "def bag_of_words(sentence, words, show_details=True):\n",
    "    # tokenizing patterns\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - vocabulary matrix\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % word)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence):\n",
    "    # filter below  threshold predictions\n",
    "    p = bag_of_words(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sorting strength probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['intent']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "#Creating tkinter GUI\n",
    "import tkinter\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "\n",
    "    if msg != '':\n",
    "        ChatBox.config(state=NORMAL)\n",
    "        ChatBox.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatBox.config(foreground=\"#446665\", font=(\"Verdana\", 12 )) \n",
    "\n",
    "        ints = predict_class(msg)\n",
    "        res = getResponse(ints, intents)\n",
    "\n",
    "        ChatBox.insert(END, \"Bot: \" + res + '\\n\\n')           \n",
    "\n",
    "        ChatBox.config(state=DISABLED)\n",
    "        ChatBox.yview(END)\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Chatbot\")\n",
    "root.geometry(\"800x500\")\n",
    "root.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "#Create Chat window\n",
    "ChatBox = Text(root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "ChatBox.config(state=DISABLED)\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = Scrollbar(root, command=ChatBox.yview, cursor=\"heart\")\n",
    "ChatBox['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = Button(root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#f9a602\", activebackground=\"#3c9d9b\",fg='#000000',\n",
    "                    command= send )\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = Text(root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
    "#EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "#Place all components on the screen\n",
    "scrollbar.place(x=776,y=6, height=386)\n",
    "ChatBox.place(x=6,y=6, height=386, width=770)\n",
    "EntryBox.place(x=128, y=401, height=90, width=765)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amd1dQvt1JHa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
